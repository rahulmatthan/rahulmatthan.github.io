<!doctype html><html class=no-js lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Moderating Systems - Ex Machina</title><script>(function(e,t){e[t]=e[t].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:url" content="https://exmachina.in/10/05/2023/moderating-systems/"><meta property="og:site_name" content="Ex Machina"><meta property="og:title" content="Moderating Systems"><meta property="og:description" content="Content moderation challenges arise from the vast volume of online content and diverse user beliefs. Current moderation uses automated tools and human moderators, but both have flaws. Evelyn Douek suggests a “systems thinking” approach, focusing on systemic solutions rather than individual errors. As India drafts the Digital India Act, a shift towards addressing systemic issues in content moderation is essential."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-05-10T00:00:00+00:00"><meta property="article:modified_time" content="2023-05-10T00:00:00+00:00"><meta property="article:tag" content="Content Moderation"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class="logo logo--mixed"><a class=logo__link href=/ title="Ex Machina" rel=home><div class="logo__item logo__imagebox"><img class=logo__img src=/images/exmachina.jpg></div><div class="logo__item logo__text"><div class=logo__title>Ex Machina</div><div class=logo__tagline>Law. Technology. Society.</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/index/><span class=menu__text>Index</span></a></li><li class=menu__item><a class=menu__link href=/topics/><span class=menu__text>Topics</span></a></li><li class=menu__item><a class=menu__link href=/books/><i class='fa fa-road'></i>
<span class=menu__text>Books</span></a></li><li class=menu__item><a class=menu__link href=/podcast/><i class='fa fa-road'></i>
<span class=menu__text>Podcast</span></a></li><li class=menu__item><a class=menu__link href=/about/><i class='fa fa-road'></i>
<span class=menu__text>About</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>Moderating Systems</h1><div class="post__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0a14 14 0 110 28 1 1 0 010-28m0 3a3 3 0 100 22 3 3 0 000-22m1 4h-2v8.4l6.8 4.4L22 18l-6-3.8z"/></svg><time class=meta__text datetime=2023-05-10T00:00:00Z>May 10, 2023</time></div></div></header><div class="content post__content clearfix"><figure><img src=/images/moderating-systems.png width=auto></figure><p><em>Content moderation challenges arise from the vast volume of online content and diverse user beliefs. Current moderation uses automated tools and human moderators, but both have flaws. Evelyn Douek suggests a &ldquo;systems thinking&rdquo; approach, focusing on systemic solutions rather than individual errors. As India drafts the Digital India Act, a shift towards addressing systemic issues in content moderation is essential.</em></p><p><em>This article was first published in The Mint. You can read the original at <a href=https://www.livemint.com/opinion/columns/content-moderation-a-systems-thinking-approach-to-addressing-the-flaws-in-modern-data-governance-and-regulatory-supervision-11683655089499.html>this link</a>.</em></p><hr><p>Content moderation is one of the more vexatious problems of modern data governance. Given the sheer volume of content generated online, it is virtually impossible to monitor everything that is said. And since users come from across the spectrum of humanity, they have a wide diversity of cultural, social and political beliefs. All this makes it extremely difficult to assess what might cause offence and to whom. Since content moderators have to strike a balance between removing illegal content and upholding users’ rights to freedom of speech, figuring out what to do is often a tightrope walk fraught with tension.</p><p>Internet platforms have tried to address the problem by building a range of systems and processes. They use automated tools to filter out obviously offensive content. Where the algorithm cannot conclusively determine whether or not a given piece of content is offensive, it escalates the decision to a team of human moderators. These teams, in turn, rely on highly detailed moderation manuals and precedents to evaluate whether or not a given item of content should stay up or not.</p><p>But even this is not without flaws. Automated tools are nowhere near as accurate as one might think they would be by now. As a result, perfectly acceptable content gets automatically removed more often than should be the case. At the same time, offensive material remains online for longer than is necessary simply because machines are unable to fully grasp the context and complexity of human thought.</p><p>As for human moderators, they each bring their own unique biases to the table. As a result, no matter how detailed the content moderation manual might be, many of their moderation decisions lack the level of impartiality needed for them to be fair.</p><p>Regulators around the world are struggling for a solution to the problem. Their approach so far has been to hold companies liable for their errors—blaming them for keeping content up too long or not taking it down as quickly as they should have. This approach, according to Evelyn Douek, is misguided since most failures in content moderation relate less to the actual incident and more to flaws in the way that the actual moderation system is built. Instead of trying to fix the consequences of individual moderation decisions, she argues, we need to focus on the upstream design choices that were the reason why those mistakes were made in the first place.</p><p>To do this, we will need to change the way we currently do things. Instead of trying to scale traditional moderation workflows, she suggests we deploy a “systems thinking" approach. This will necessarily call for structural changes to the organisations responsible for content moderation. They will need to put in place robust ‘Chinese walls’ between those in the organization who are responsible for the enforcement of moderation rules and other functions such as product development, customer growth and political lobbying. More often than not, the content moderation decisions in these companies are hijacked by commercial considerations thrown up by other departments.</p><p>Douek also suggests that we shift the focus of complaints away from demanding redress for specific moderation decisions to finding broader systemic solutions to address the underlying flaws in the moderation systems. This, she points out, will generate a far more effective set of changes that will benefit the larger community of users in a more substantial way.</p><p>All this needs to be accompanied by a brand new approach to regulatory supervision. Instead of sitting in judgment over individual moderation decisions, regulators need to take a more systemic view of the problem. They need to first put in place audit mechanisms that will help assess whether the overall content management strategy is capable of adequately addressing the moderation challenges it will face. Europe’s Digital Services Act, for instance, requires large platforms to prepare annual risk assessments that list the systemic risks that are likely to occur and explain how these risks could affect their moderation systems.</p><p>Instead of focusing on individual instances of questionable moderation, regulators should require platforms to analyse their decisions in the aggregate—for instance, all adverse decisions in a given category of rule violation over a one-year period instead of each individual one. This sort of an approach will have more long lasting, system-wide benefits that will improve future outcomes.</p><p>To be clear, Douek’s approach does have its share of sceptics. Mariela Olivares, in her paper, ‘Of Systems Thinking and Straw Men’, argues that Douek has oversimplified the complexities involved in moderation. Focusing exclusively on systems thinking could, she argues, end up reinforcing existing power structures and biases, instead of offering practical solutions.</p><p>That said, this is the sort of thinking we need to adopt if we are to have any hope of finding a viable solution. There is little argument that the current system is broken, and unless we take a different approach, we will struggle for a good response.</p><p>As India begins work on the draft Digital India Act, I hope that we will remain open to novel alternative approaches. Rather than focusing our regulatory efforts on how to best adjudicate individual moderation decisions, we need to put in place substantive, long-term solutions that address the systemic problems at the heart of the issue.</p></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M4 0h8s2 0 4 2l15 15s2 2 0 4L21 31s-2 2-4 0L2 16s-2-2-2-4V3s0-3 4-3m3 10a3 3 0 000-6 3 3 0 000 6"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/content-moderation/ rel=tag>Content Moderation</a></li></ul></div></footer></article></main><div class="authorbox clearfix"><p class=authorbox__warning><strong>WARNING:</strong> Authorbox is activated, but [Author] parameters are not specified.</p></div><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=/03/05/2023/digital-gender-inclusion/ rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>Digital Gender Inclusion</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=/17/05/2023/ill-advised-advisory/ rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>Ill-Advised Advisory</p></a></div></nav></div><aside class=sidebar></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2025 Ex Machina.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script></body></html>