---
title: "Tools For Thought"
date: "2020-12-09"
tags: ["Technology"]
widgets: 
- "categories"
---

*Earlier this year, as news of the incredibly realistic prose that GPT-3 was capable of generating reached me, my first [thought](https://twitter.com/matthan/status/1285029141956288514?s=20) was that I needed to figure out how to use this technology to churn out new Ex Machina articles. It wasn’t long before I realised that as a technology, it flattered to deceive. I would be far better served investing in consciously building a personal knowledge management system that help me synthesise all the various elements of knowledge I came across so that I was able to surface hidden connections.*

<!--more-->

*This article was first published in The Mint. You can read the original at [this link](https://www.livemint.com/opinion/columns/zettlekasten-and-food-for-thought-served-by-technology-11607445278979.html).*

---

I have often been asked how I manage to find a new topic to write on every week. Truth be told, it is hard work. It helps that I read a lot on a wide range of diverse topics, and that I have worked at the intersection of law and society for over two decades. So there are a fair number of experiences I can draw upon to help place current legislative developments in a historical context. But even so, trying to find new perspectives to take and write about is hard work, and I wish there was a reliable technological solution I could use to help me do it.

### Can Computers Write Prose?

Earlier this year, I, along with the rest of the world, was blown away by a new artificial intelligence (AI) system that seemed to have an almost human-like facility with the English language. It was called [GPT-3](https://arxiv.org/abs/2005.14165) and was the latest iteration of the machine-learning language system developed by [OpenAI](https://openai.com/) that was capable of generating text so coherent that it was indistinguishable from human prose. So impressive was this technology that some of the early articles describing GPT-3 were written by the AI engine itself, and it was not until the end of the article, when readers were informed they had been reading the words of a machine, that it dawned on them that they had been reading computer-generated text.

Almost at once, examples of the many uses to which such technology could be put were described. Some argued that once it becomes possible for us to converse with computers at a conceptual level, we could find answers to philosophical questions about the existence of [God](https://medium.com/merzazine/about-humans-ai-and-god-1a7e7cf8e8cf) and the meaning of life. Others pointed out that GPT-3 could be used to provide a medical diagnosis or serve as a [therapist](https://twitter.com/nickcammarata/status/1283300424418619393). There were also some who began to conceptualize a [future without keyboards](https://twitter.com/azlenelza/status/1331623011049500678) — where we would use a touch interface to massage prose into a form of our choice, just like we manipulate images today with photo-editing software.

Despite its promise, GPT-3 is not the solution I’ve been looking for. Don’t get me wrong. It is an impressive step forward, showing us just how far computers have come in their ability to work with words. But as impressive as it is, GPT-3 is little more than a glorified auto-complete program that generates essay-length suggestions of appropriate text in much the same way that our mobile operating systems suggest short sentence responses to the text messages we receive. The text it produces makes coherent sense not because it understands any better the meaning of the words it uses, or their conceptual context, but because it is really good at identifying sentence patterns and using those to generate other sentences that make contextual sense. This is not intelligence, just an impressive parlour trick.

### Managing Knowledge

The internet has brought all the world’s knowledge within our grasp. But access to information is just the first step. What we need next are tools to reveal connections hidden deep within that knowledge. As good as machines have become at predicting patterns, they are still hopeless at connecting the dots. That is why a tool like GPT-3 is not really useful for what I want to do. What I need instead is technology that helps me think.

[Niklas Luhmann](https://en.wikipedia.org/wiki/Niklas_Luhmann) was a German scholar who in the course of his career wrote 70 books and over 400 scholarly articles. The secret behind this prodigious output was a technique that he called [Zettlekasten](https://medium.com/@rebeccawilliams9941/the-zettelkasten-method-examples-to-help-you-get-started-8f8a44fa9ae6) — a systematic note-taking workflow that has all of a sudden become a rage among some of the world’s top knowledge warriors.

Described simply, Zettlekasten is a process designed to bring to the surface connections between disparate pieces of knowledge by taking precise, atomic notes and systematically indexing and tagging them so that they are relevant in all the different intellectual contexts they might be put to use.

Luhmann made notes of everything he read, but, unlike the rest of us, also linked each note with those he had made before. This way, every item of new information appropriately re-surfaced all the connected pieces of past knowledge that he had accumulated—even if that past knowledge had been gathered in a completely different context.

It is this ability to access undiscovered connections that was the secret of his prolific literary output. It is this magic that some of the latest, most cutting-edge personal knowledge management tools are trying to replicate in code.

### Tools for Thought

For nearly a year now, I have been using one of them—an application called [Roam Research](https://roamresearch.com/) — for all my knowledge management needs. Roam allows me to take free-form notes and then, using a technique called back-linking, lets me link those notes to every other note related to the topic that I have made before.

As with all such technologies, it takes a while before the effort you put in starts to yield results. But once the volume of notes in your personal knowledge graph crosses a critical threshold, the connections begin to magically surface on their own. This is how I realized that an article I had read a year ago on the history of classical music contained examples I could use to argue that we needed to bring some form of standardization to our notions of [internet governance](https://www.livemint.com/opinion/columns/lessons-on-internet-governance-from-the-concert-hall-11580229305635.html). And how the observations that Thomas Edison made in the context of battery-operated cars could be used to illustrate the concept of [few-shot learning](https://exmachina.substack.com/p/we-dont-need-large-datasets).

It might be a while before computers can actually think for themselves. Until then, we can—and should—use them as tools for human thought.