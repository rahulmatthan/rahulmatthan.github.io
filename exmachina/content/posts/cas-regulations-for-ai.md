---
title: "CAS Regulations for AI"
date: "2024-02-14"
tags: ["artificial intelligence"]
widgets: ["categories"]
---

{{< figure src="/images/ey.jpg" width="auto" >}}

_The PM-EAC suggests that AI should be regulated as a complex adaptive system. While there is a lot to say about this approach, in its articulation, the paper fails to take into account many of the essential features of modern AI._

<!--more-->

_This article was first published in The Mint. You can read the original at [_this link_](https://www.livemint.com/opinion/online-views/a-good-way-to-regulate-ai-is-to-think-of-it-as-a-complex-adaptive-system-11707837055204.html)._

___

Last month, the Prime Minister’s Economic Advisory Council (PM-EAC) released a [paper](https://eacpm.gov.in/wp-content/uploads/2024/01/EACPM_AI_WP-1.pdf) proposing a new approach to regulating Artificial Intelligence (AI). It argues that while our current approach of enacting reactionary regulations might work in a static, linear system with predictable risks, it is unlikely to work in the context of AI, which comprises emergent, non-linear systems. It argues that since AI is a dynamic network of diverse agents whose interactions generate emergent behaviours, we need to think of it as a complex adaptive system (CAS) and design regulations accordingly.

### Regulating CAS

We already have experience dealing with complex adaptive systems like [stock markets](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1745-6622.2002.tb00448.x). The paper attempts to extract regulatory principles from those systems, so that we can apply them to AI. For instance, it suggests that we put in place [guardrails](https://opendatascience.com/how-to-use-guardrails-to-design-safe-and-trustworthy-ai/) and partitions to define operational spaces within which AI can operate, so that, if needed, we can be sure it will not accidentally stray into potentially hazardous areas. It also calls for building [manual overrides](https://www.nationalacademies.org/news/2022/04/ensuring-human-control-over-ai-infused-systems) and authorization choke-points directly into these AI systems, so that humans can effectively take control of operations where needed. It makes the case for “[transparency and explainability](https://oecd.ai/en/dashboards/ai-principles/P7),” so that there will always be public scrutiny of these systems. The PM-EAC paper also suggests that we ensure “distinct [accountability](https://link.springer.com/article/10.1007/s00146-023-01635-y),” so we can always identify the entity or individual responsible for any unintended consequence. Finally, the paper recommends that we put in place an AI regulator with the expertise and the mandate to recalibrate regulations on the fly to deal with the dynamic requirements of CAS regulation.

If nothing else, categorizing AI as a CAS is a refreshingly novel approach to finding solutions for a particularly challenging problem. AI policies tend to be knee-jerk responses to manifestations of harms resulting from the use of this technology, but regulators using this [whack-a-mole approach](https://www.brookings.edu/articles/the-three-challenges-of-ai-regulation/) will constantly find themselves behind the curve. By taking a step back and thinking of the entire AI landscape as a CAS that displays emergent behaviour and is continuously and spontaneously evolving is an effective way to arrive at a workable long-term solution.

For that reason, I agree with the proposal to put in place a dedicated and [agile](https://www.weforum.org/agenda/2023/11/its-time-we-embrace-an-agile-approach-to-regulating-ai/) expert regulatory body with the power to issue directions and amend regulations on-the-fly as and when required. If such a regulator is obliged to operate in accordance with a set of principles aligned with the democratic values of the country, I see this as no different from the principles-based regulation approach that I have called for in earlier [articles](https://exmachina.in/02/02/2021/principle-based-regulations/) in this column.

### Partitions and Accountability

That said, there is much that I disagree with in the paper, such as the idea of guardrails and partitions that it suggests. While the concept itself may be sound, given the way in which AI has already been built so far, this suggestion would be virtually impossible to implement. Much of AI development has been [modular](https://sapphireventures.com/blog/the-future-of-ai-infrastructure-is-becoming-modular/—particularly lower down the stack, with customization mainly taking place at higher levels. 

As a consequence, for all intents and purposes, this is a ship that might have already sailed. While it is still possible to ‘[air-gap](https://www.howtogeek.com/687792/the-ultimate-defense-what-is-an-air-gapped-computer/’ some core systems—think of weapons, power grids, etc—doing so will force us to build them from scratch, and, as a result, forgo many of the benefits that could have come from building on top of what has already been developed.

This is also why the goal of ensuring “distinct accountability” might be impossible to achieve. Today, AI is designed to be [interoperable](https://www.clickworker.com/customer-blog/interoperability-and-the-future-of-machine-learning/), with access provided through application programming interfaces (APIs) designed for deep integration of AI into other digital products. Technologies like [IFTTT](https://ifttt.com/) and [Zapier](https://zapier.com/) take this interoperability even further by allowing do-it-yourself combinations of services without any need for coding expertise. All of which is to say that even though we are at the dawn of the AI age, it may already be impossible to pin distinct responsibility for AI outcomes on a single individual or entity.

### Oversight and Explainability

I am just as sceptical of the paper’s blind insistence on human oversight and the need always to have [humans in the loop](https://levity.ai/blog/human-in-the-loop). One of the reasons why we moved to automation in the first place was to avoid [biased human decision-makers](https://www.psychologytoday.com/us/blog/thoughts-on-thinking/201809/12-common-biases-that-affect-how-we-make-everyday-decisions). Now that we have committed ourselves to this path and integrated machines into our workflows, we have reached a point where humans can no longer keep up. Our [poorer senses](https://www.nature.com/articles/d41586-019-03847-z) and [slower reaction times](https://www.investopedia.com/financial-edge/0113/has-high-frequency-trading-ruined-the-stock-market-for-the-rest-of-us.aspx) mean that we are [no match for our machine counterparts](https://theconversation.com/digital-diagnosis-intelligent-machines-do-a-better-job-than-humans-53116).

And then there is the demand for transparency and explainability. As I have argued before, whenever we insist on transparency, it is often in [exchange for performance](https://arxiv.org/pdf/2307.14239.pdf),  and while in certain circumstances—such as where human life and liberty are at stake—this might be appropriate, in others, it will not be. For instance, AI can analyse radiology images with far greater accuracy than humans. If this gives me a better chance at detecting a potentially fatal disease, I don’t see why I should give this up simply because we need algorithms to be explainable.

Using a CAS approach to formulate regulations for AI is indeed novel and refreshing as a way to solve a wicked problem. But we cannot blindly apply these regulatory principles to AI without a proper understanding of how it will impact operations in this important sector. Instead, we should work at adapting CAS principles so that we can achieve the desired outcomes.